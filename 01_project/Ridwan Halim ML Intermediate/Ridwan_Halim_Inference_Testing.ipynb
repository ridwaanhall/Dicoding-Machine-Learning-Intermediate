{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ridwan Halim"
      ],
      "metadata": {
        "id": "5urZLyf4g9Jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "M4e209f4b_uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "qkFL9zRVU_LM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download model from my github: [ridwaanhall](https://github.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/tree/main/01_project/03_model)"
      ],
      "metadata": {
        "id": "4jv0-6szcCUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O xgboost_word2vec_model.joblib https://github.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/raw/main/01_project/03_model/xgboost_word2vec_model.joblib\n",
        "!wget -O word2vec_model.joblib https://github.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/raw/main/01_project/03_model/word2vec_model.joblib\n",
        "!wget -O label_encoder.joblib https://github.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/raw/main/01_project/03_model/label_encoder.joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvorZkjubtZu",
        "outputId": "e04a20bf-985a-48a4-986a-bb397cb4bf3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-21 13:15:39--  https://github.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/raw/main/01_project/03_model/xgboost_word2vec_model.joblib\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/main/01_project/03_model/xgboost_word2vec_model.joblib [following]\n",
            "--2024-07-21 13:15:40--  https://raw.githubusercontent.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/main/01_project/03_model/xgboost_word2vec_model.joblib\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1173433 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘xgboost_word2vec_model.joblib’\n",
            "\n",
            "xgboost_word2vec_mo 100%[===================>]   1.12M  6.14MB/s    in 0.2s    \n",
            "\n",
            "2024-07-21 13:15:41 (6.14 MB/s) - ‘xgboost_word2vec_model.joblib’ saved [1173433/1173433]\n",
            "\n",
            "--2024-07-21 13:15:41--  https://github.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/raw/main/01_project/03_model/word2vec_model.joblib\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/main/01_project/03_model/word2vec_model.joblib [following]\n",
            "--2024-07-21 13:15:42--  https://raw.githubusercontent.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/main/01_project/03_model/word2vec_model.joblib\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14772483 (14M) [application/octet-stream]\n",
            "Saving to: ‘word2vec_model.joblib’\n",
            "\n",
            "word2vec_model.jobl 100%[===================>]  14.09M  36.7MB/s    in 0.4s    \n",
            "\n",
            "2024-07-21 13:15:44 (36.7 MB/s) - ‘word2vec_model.joblib’ saved [14772483/14772483]\n",
            "\n",
            "--2024-07-21 13:15:44--  https://github.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/raw/main/01_project/03_model/label_encoder.joblib\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/main/01_project/03_model/label_encoder.joblib [following]\n",
            "--2024-07-21 13:15:44--  https://raw.githubusercontent.com/ridwaanhall/Dicoding-Machine-Learning-Intermediate/main/01_project/03_model/label_encoder.joblib\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 559 [application/octet-stream]\n",
            "Saving to: ‘label_encoder.joblib’\n",
            "\n",
            "label_encoder.jobli 100%[===================>]     559  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-21 13:15:45 (30.3 MB/s) - ‘label_encoder.joblib’ saved [559/559]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentimentAnalyzer"
      ],
      "metadata": {
        "id": "Pw_hzk2vcISp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djxFvislUWVe",
        "outputId": "296fdb8e-f95e-4070-a572-37e7939d1ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:16:09] WARNING: /workspace/src/common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
            "configuration generated by an older version of XGBoost, please export the model by calling\n",
            "`Booster.save_model` from that version first, then load it back in current version. See:\n",
            "\n",
            "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
            "\n",
            "for more details about differences between saving model and serializing.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "class SentimentAnalyzer:\n",
        "    def __init__(self, model_path, word2vec_path, label_encoder_path):\n",
        "        self.model = joblib.load(model_path)\n",
        "        self.word2vec_model = joblib.load(word2vec_path)\n",
        "        self.label_encoder = joblib.load(label_encoder_path)\n",
        "\n",
        "    def vectorize(self, text):\n",
        "        \"\"\"Vectorize text using Word2Vec.\"\"\"\n",
        "        vectors = [self.word2vec_model.wv[word] for word in text.split() if word in self.word2vec_model.wv]\n",
        "        if len(vectors) == 0:\n",
        "            return np.zeros(self.word2vec_model.vector_size)\n",
        "        return np.mean(vectors, axis=0)\n",
        "\n",
        "    def predict_proba(self, text):\n",
        "        \"\"\"Predict the sentiment probabilities for the input text.\"\"\"\n",
        "        input_vector = self.vectorize(text).reshape(1, -1)  # Reshape for the model\n",
        "        probabilities = self.model.predict_proba(input_vector)[0]\n",
        "        return probabilities\n",
        "\n",
        "    def get_percentage_predictions(self, text):\n",
        "        \"\"\"Get sentiment predictions with percentages.\"\"\"\n",
        "        probabilities = self.predict_proba(text)\n",
        "        class_labels = self.label_encoder.classes_\n",
        "        percentages = {label: prob * 100 for label, prob in zip(class_labels, probabilities)}\n",
        "        return percentages\n",
        "\n",
        "    def print_predictions(self, text):\n",
        "        \"\"\"Print sentiment predictions sorted from highest to lowest.\"\"\"\n",
        "        percentages = self.get_percentage_predictions(text)\n",
        "        # Sort\n",
        "        sorted_percentages = dict(sorted(percentages.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "        # Determine the predicted class\n",
        "        predicted_class = max(sorted_percentages, key=sorted_percentages.get)\n",
        "\n",
        "        print(f\"Your text: {text}\")\n",
        "        print(f\"Predict: {predicted_class.capitalize()}\")\n",
        "        print(\"\\nDetail:\")\n",
        "        for label, percentage in sorted_percentages.items():\n",
        "            print(f\"{label.capitalize()}: {percentage:.5f}%\")\n",
        "\n",
        "model_path = 'xgboost_word2vec_model.joblib'\n",
        "word2vec_path = 'word2vec_model.joblib'\n",
        "label_encoder_path = 'label_encoder.joblib'\n",
        "\n",
        "analyzer = SentimentAnalyzer(model_path, word2vec_path, label_encoder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "qg2SNsWtcNgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative"
      ],
      "metadata": {
        "id": "mMsO_RDmdLIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"harga barangnya mahal mahal\"\n",
        "analyzer.print_predictions(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GGE_pQcb8_s",
        "outputId": "17066d06-5e10-4cbb-f5a3-2b6793cdc2b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your text: harga barangnya mahal mahal\n",
            "Predict: Negatif\n",
            "\n",
            "Detail:\n",
            "Negatif: 90.94258%\n",
            "Positif: 8.83364%\n",
            "Netral: 0.22378%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Pengalaman belanja di Tokopedia kurang memuaskan. Barang yang saya pesan tidak sesuai dengan deskripsi. Pelayanan pelanggan juga lambat dalam merespons keluhan.\"\n",
        "analyzer.print_predictions(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_52LXWOd4Oy",
        "outputId": "aaf9b760-0cfd-4856-929b-ea08448f46ad"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your text: Pengalaman belanja di Tokopedia kurang memuaskan. Barang yang saya pesan tidak sesuai dengan deskripsi. Pelayanan pelanggan juga lambat dalam merespons keluhan.\n",
            "Predict: Negatif\n",
            "\n",
            "Detail:\n",
            "Negatif: 82.37570%\n",
            "Positif: 17.58235%\n",
            "Netral: 0.04196%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positve"
      ],
      "metadata": {
        "id": "yKuVK6LkdaD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"harga barangnya murah murah\"\n",
        "analyzer.print_predictions(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0ax7weTce7b",
        "outputId": "3eb68416-a36a-4e6d-da84-2ae8b7728feb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your text: harga barangnya murah murah\n",
            "Predict: Positif\n",
            "\n",
            "Detail:\n",
            "Positif: 98.47952%\n",
            "Negatif: 1.48496%\n",
            "Netral: 0.03553%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Aplikasi Tokopedia sangat mudah digunakan dan selalu memberikan promo menarik! Pengalaman belanja jadi lebih menyenangkan dan hemat. Pengiriman cepat dan customer service juga sangat responsif. Terima kasih, Tokopedia!\"\n",
        "analyzer.print_predictions(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7Z119raeLlp",
        "outputId": "35848ee1-336d-4d1f-e872-0a854b00cc9f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your text: Aplikasi Tokopedia sangat mudah digunakan dan selalu memberikan promo menarik! Pengalaman belanja jadi lebih menyenangkan dan hemat. Pengiriman cepat dan customer service juga sangat responsif. Terima kasih, Tokopedia!\n",
            "Predict: Positif\n",
            "\n",
            "Detail:\n",
            "Positif: 99.99403%\n",
            "Negatif: 0.00457%\n",
            "Netral: 0.00140%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neutral"
      ],
      "metadata": {
        "id": "NxJvDFBVd0sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"jangan lupa saksikan iklan kami di tokopedia\"\n",
        "analyzer.print_predictions(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDtM0S1XchP9",
        "outputId": "be2b8ceb-2dd2-47be-bd04-2bccac57e576"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your text: jangan lupa saksikan iklan kami di tokopedia\n",
            "Predict: Netral\n",
            "\n",
            "Detail:\n",
            "Netral: 60.01596%\n",
            "Negatif: 30.59819%\n",
            "Positif: 9.38585%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"jangan lupa berbelanja\"\n",
        "analyzer.print_predictions(input_text)\n",
        "\n",
        "# positive but have high neutral predict."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJDxGhyEfHxk",
        "outputId": "d609a79a-69f6-453b-bb7a-1e0c177db6a7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your text: jangan lupa berbelanja\n",
            "Predict: Positif\n",
            "\n",
            "Detail:\n",
            "Positif: 50.69026%\n",
            "Netral: 48.54724%\n",
            "Negatif: 0.76250%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## boleh dicoba lho kakak kakak reviewer :) tinggal run run masukin text. tapi terkadang masih kgak akurat"
      ],
      "metadata": {
        "id": "lCyvL-OLgnog"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G8BcysfUfJ7M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}